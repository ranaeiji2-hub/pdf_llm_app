import faiss
import pickle
import requests
import numpy as np

# push 02210933

# ---------- Ollama ----------
OLLAMA_EMBED_URL = "http://host.docker.internal:11434/api/embeddings"
OLLAMA_CHAT_URL  = "http://host.docker.internal:11434/api/chat"

EMBED_MODEL = "nomic-embed-text"
CHAT_MODEL  = "qwen2.5:3b-instruct"

# ---------- RAG ----------
INDEX_PATH = "index.faiss"
TEXTS_PATH = "texts.pkl"
TOP_K = 3

# ---------- Embedding ----------
def embed(text: str) -> np.ndarray:
    r = requests.post(
        OLLAMA_EMBED_URL,
        json={"model": EMBED_MODEL, "prompt": text},
        timeout=60
    )
    return np.array(r.json()["embedding"], dtype="float32")

# ---------- LLM ----------
def ask_llm(prompt: str) -> str:
    r = requests.post(
        OLLAMA_CHAT_URL,
        json={
            "model": CHAT_MODEL,
            "stream": False,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "options": {
                "num_predict": 256,
                "temperature": 0.1,
                "top_k": 2,
                "top_p": 0.8
            }
        },
        timeout=180
    )
    data = r.json()
    return data["message"]["content"]

# ---------- Load ----------
index = faiss.read_index(INDEX_PATH)
with open(TEXTS_PATH, "rb") as f:
    texts = pickle.load(f)

print("ğŸš€ Fast RAG readyï¼ˆexitã§çµ‚äº†ï¼‰")

# ---------- Loop ----------
while True:
    query = input("\n> ")
    if query.lower() in ("exit", "quit"):
        break

    # 1ï¸âƒ£ Embedding
    qvec = embed(query).reshape(1, -1)

    # 2ï¸âƒ£ FAISSã§é–¢é€£è³‡æ–™å–å¾—
    _, ids = index.search(qvec, TOP_K)
    context = "\n\n".join(texts[i] for i in ids[0])

    # 3ï¸âƒ£ å˜ä¸€è³ªå•ç”¨ prompt
    prompt = f"""
ä»¥ä¸‹ã®è³‡æ–™ã‚’ã‚‚ã¨ã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
è³‡æ–™ã«ç„¡ã„ã“ã¨ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

### è³‡æ–™
{context}

### è³ªå•
{query}
"""

    # 4ï¸âƒ£ LLMã«å•ã„åˆã‚ã›
    answer = ask_llm(prompt)

    # 5ï¸âƒ£ ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
    print(answer)
